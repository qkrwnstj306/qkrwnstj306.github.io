<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Junseo Park</title>
    <description>devlopr-jekyll is a beautiful Jekyll Theme Built For Developers, which is optimized for speed and readability.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 01 Jan 2025 16:24:56 +0900</pubDate>
    <lastBuildDate>Wed, 01 Jan 2025 16:24:56 +0900</lastBuildDate>
    <generator>Jekyll v4.3.4</generator>

    
      <item>
        <title>I2AM: Interpreting Image-to-Image Latent Diffusion Models via Bi-Attribution Maps</title>
        <description>&lt;div align=&quot;center&quot; style=&quot;
  font-family: &apos;Times New Roman&apos;, Times, serif;
  font-size: 20px;
  font-weight: bold;
  color: #4a4a4a;
  padding: 20px;
  margin: 20px auto;
  border: 2px solid #e0e0e0;
  border-radius: 10px;
  background: linear-gradient(120deg, #f0f8ff, #ffffff);
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);&quot;&gt;
  üöÄ &quot;Interpreting Image-to-Image LDMs via Bi-Attribution Maps&quot; üåü
&lt;/div&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Large-scale diffusion models have achieved significant advancements in image generation, particularly through cross-attention mechanisms.&lt;/li&gt;
  &lt;li&gt;While cross-attention has been extensively studied in text-to-image tasks &lt;em&gt;(e.g., DAAM)&lt;/em&gt;, its interpretability in image-to-image (I2I) diffusion models remains underexplored. To address this, we introduce Image-to-Image Attribution Maps ($\text{I}^{2}\text{AM}$).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;problem&quot;&gt;Problem&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The interpretability of I2I diffusion models is insufficiently explored.&lt;/li&gt;
  &lt;li&gt;Applying text-to-image attribution/attention methods to I2I models often results in uninterpretable maps due to the contextual continuity of reference images &lt;em&gt;(vs. DAAM)&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;proposed-method&quot;&gt;Proposed Method&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;We propose a bidirectional attribution analysis guided by two key research questions:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Q1&lt;/strong&gt;: ‚ÄúWhich regions of the generated image are most influenced by the reference image?‚Äù ‚Äî exploring how the model utilizes the reference image during generation.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Q2&lt;/strong&gt;: ‚ÄúWhich parts of the reference image contribute most to the generated image?‚Äù ‚Äî assessing how well the model captures and transfers critical details from the input.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/i2am_2.png&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\text{I}^{2}\text{AM}$ aggregates cross-attention scores across time steps, attention heads, and layers to provide insights into feature transfer between images:
    &lt;ul&gt;
      &lt;li&gt;ULAM (Unified-Level Attribution Map): Integrates time, head, and layer scores to reveal overall trends.&lt;/li&gt;
      &lt;li&gt;TLAM (Time-Level Attribution Map): Tracks image generation processes over time.&lt;/li&gt;
      &lt;li&gt;HLAM (Head-Level Attribution Map): Analyzes attention score distributions across heads, highlighting diverse information capture.&lt;/li&gt;
      &lt;li&gt;LLAM (Layer-Level Attribution Map): Explains how input features are processed and transformed at each layer, guiding model performance improvement.&lt;/li&gt;
      &lt;li&gt;SRAM (Specific-Reference Attribution Map): Highlights regions in the reference image contributing to specific patches in the generated image.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The top attribution map shows how different areas of the generated image are influenced by the reference image &lt;strong&gt;(Q1)&lt;/strong&gt;. Conversely, the bottom map demonstrates how different regions of the reference image contribute to the generation process &lt;strong&gt;(Q2)&lt;/strong&gt;, and the right map illustrates which reference information was extracted during the generation of that cell.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/i2am_1.png&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\text{I}^{2}\text{AM}$ demonstrates effectiveness across object detection, inpainting, and super-resolution tasks, successfully identifying key regions critical to output generation even in complex scenes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/i2am_3.png&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/i2am_4.png&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We also introduce the &lt;strong&gt;Inpainting Mask Attention Consistency Score (IMACS)&lt;/strong&gt;, a novel metric that evaluates alignment between attribution maps and inpainting masks, showing strong correlation with existing performance metrics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/i2am_5.png&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Extensive experiments validate that $\text{I}^{2}\text{AM}$ supports model debugging and refinement, enhancing both the performance and interpretability of I2I models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/i2am_6.png&quot; /&gt;
&lt;/p&gt;
</description>
        <pubDate>Wed, 02 Oct 2024 18:05:23 +0900</pubDate>
        <link>http://localhost:4000/blog/i2am/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/i2am/</guid>
      </item>
    
      <item>
        <title>StyleForge: Enhancing Text-to-Image Synthesis for Any Artistic Styles with Dual Binding</title>
        <description>&lt;div align=&quot;center&quot; style=&quot;
  font-family: &apos;Times New Roman&apos;, Times, serif;
  font-size: 20px;
  font-weight: bold;
  color: #4a4a4a;
  padding: 20px;
  margin: 20px auto;
  border: 2px solid #e0e0e0;
  border-radius: 10px;
  background: linear-gradient(120deg, #f0f8ff, #ffffff);
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);&quot;&gt;
  üöÄ &quot;Advanced Version of StyleBoost&quot; üåü
&lt;/div&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Recent advancements in text-to-image models, such as Stable Diffusion, have demonstrated the ability to generate visual images from natural language prompts.&lt;/li&gt;
  &lt;li&gt;This progress has also driven the development of personalization techniques for binding user inputs (e.g., images) to prompts, including methods like DreamBooth, Textual Inversion, and LoRA.&lt;/li&gt;
  &lt;li&gt;However, these approaches struggle to capture arbitrary artistic styles due to the abstract and multifaceted nature of stylistic attributes.&lt;/li&gt;
  &lt;li&gt;To reliably learn target styles, StyleBoost (equivalent to Single-StyleForge in this paper) proposes dual binding. It uses $15 ‚Äì 20$ target style images to bind them to a unique token identifier while leveraging auxiliary images to ensure consistent representation of critical elements like people within the target style.&lt;/li&gt;
  &lt;li&gt;StyleForge is an advanced version of &lt;a href=&quot;http://localhost:4000/blog/styleboost&quot;&gt;StyleBoost&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Improvements in StyleForge over StyleBoost:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Validation of auxiliary images through heat maps, ensuring effectiveness and practicality.&lt;/li&gt;
  &lt;li&gt;Introduction of Multi-StyleForge: instead of using a single unique token, it binds target and auxiliary images to multiple tokens, separating components like people and backgrounds.&lt;/li&gt;
  &lt;li&gt;Broader experiments covering more target styles and baselines.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;problem&quot;&gt;Problem&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;StyleBoost (or Single-StyleBoost)&lt;/strong&gt; was proposed as a method to learn artistic styles, ensuring faithful image generation. However, it was found to lack strong text-image alignment capabilities.&lt;/li&gt;
  &lt;li&gt;This paper introduces &lt;strong&gt;Multi-StyleForge&lt;/strong&gt;, which improves text-image alignment while maintaining high image quality.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;proposed-method&quot;&gt;Proposed Method&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;While Single-StyleForge focuses on learning a comprehensive representation of the target style, Multi-StyleForge enhances this by dividing stylistic attributes into distinct components. This improves alignment between text prompts and generated images, especially for complex styles involving both backgrounds and people.&lt;/li&gt;
  &lt;li&gt;Multi-StyleForge builds upon Single-StyleForge by mapping each stylistic component to a unique identifier.&lt;/li&gt;
  &lt;li&gt;Single-StyleForge maps StyleRef images to a single prompt &lt;em&gt;(e.g., ‚Äú[V] style‚Äù)&lt;/em&gt;, which often results in unintended inclusion of people in images when prompts lack person-related descriptions. Multi-StyleForge addresses this by introducing two StyleRef prompts &lt;em&gt;(e.g., ‚Äú[V] style‚Äù for people and ‚Äú[W] style‚Äù for backgrounds)&lt;/em&gt;, training the model more effectively and reducing ambiguity.&lt;/li&gt;
  &lt;li&gt;StyleRef images follow the Single-StyleForge structure, dividing target style elements into two parts: people and backgrounds. Each component is linked to a specific prompt &lt;em&gt;(e.g., ‚Äú[V] style‚Äù for people, ‚Äú[W] style‚Äù for backgrounds)&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;As a result, Multi-StyleForge trains the model to distinguish stylistic features (people and backgrounds) and achieve separate embeddings.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;

&lt;p&gt;$\textbf{Experimental setting}$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Target styles&lt;/strong&gt;: Realism, SureB, Anime, Romanticism, Cubism, and Pixel-art&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Baselines&lt;/strong&gt;: DreamBooth, Textual Inversion, LoRA, and Custom Diffusion&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Eval metrics&lt;/strong&gt;: FID, KID, and CLIP scores&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Attention maps for ‚Äú[V]‚Äù and ‚Äústyle‚Äù tokens in the prompt: ‚Äú[V]‚Äù focuses on a broader area, while ‚Äústyle‚Äù focuses on people, aligning with the design intent.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/styleforge_3.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;$\textbf{Main result}$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Quantitative comparisons with FID, KID ($\times 10^3$), and CLIP scores. The table presents FID scores for realism, SureB, and anime styles, along with
KID scores for romanticism, cubism, and pixel-art styles, and CLIP scores for all styles. The best and second-best results are indicated in bold and underline ,respectively.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/styleforge_2.png&quot; /&gt;
&lt;/p&gt;
</description>
        <pubDate>Wed, 04 Sep 2024 18:05:23 +0900</pubDate>
        <link>http://localhost:4000/blog/styleforge/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/styleforge/</guid>
      </item>
    
      <item>
        <title>Real-Time Text-to-Image synthesis with LCM-LoRA</title>
        <description>&lt;div align=&quot;center&quot; style=&quot;
  font-family: &apos;Times New Roman&apos;, Times, serif;
  font-size: 20px;
  font-weight: bold;
  color: #4a4a4a;
  padding: 20px;
  margin: 20px auto;
  border: 2px solid #e0e0e0;
  border-radius: 10px;
  background: linear-gradient(120deg, #f0f8ff, #ffffff);
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);&quot;&gt;
  üöÄ &quot;Provides text-to-image synthesis in real-time &lt;br /&gt; through LCM-LoRA&quot; üåü
&lt;/div&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The Latent Diffusion Model (LDM) has brought innovation to reliable image generation. Nevertheless, the inherently slow sampling process of diffusion models hinders real-time generation, negatively impacting user experience.&lt;/li&gt;
  &lt;li&gt;Efforts to accelerate LDM generally fall into two categories:
    &lt;ol&gt;
      &lt;li&gt;Utilizing advanced ODE solvers such as DDIM and DPMSolver, which drastically reduce the $1,000$ time steps of DDPM.&lt;/li&gt;
      &lt;li&gt;Distilling LDM.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;problem&quot;&gt;Problem&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The slow generation speed of traditional LDMs makes it challenging to satisfy user experience.&lt;/li&gt;
  &lt;li&gt;Deploying a site to test user satisfaction requires significant time and resources.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;proposed-method&quot;&gt;Proposed Method&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Introducing LCM-LoRA into LDM (&lt;a href=&quot;https://arxiv.org/abs/2307.01952&quot;&gt;SDXL&lt;/a&gt;) to enable near real-time image synthesis:
    &lt;ul&gt;
      &lt;li&gt;The recently introduced Latent &lt;a href=&quot;https://arxiv.org/abs/2310.04378&quot;&gt;Consistency Model (LCM)&lt;/a&gt;, inspired by the &lt;a href=&quot;https://arxiv.org/abs/2303.01469&quot;&gt;Consistency Model (CM)&lt;/a&gt;, allows for the application of CM in latent space.
        &lt;ul&gt;
          &lt;li&gt;It can create an origin from any point on the ODE path.&lt;/li&gt;
          &lt;li&gt;This enables efficient synthesis of high-resolution images with as few as $1$ to $4$ time steps.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;LCM-LoRA acts as an adapter that can be attached to existing LDMs, reducing training costs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Building a website using Flask and ChatGPT.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;As demonstrated in the demo video on &lt;a href=&quot;https://github.com/qkrwnstj306/Text-to-Image-Toy-Project/&quot;&gt;Github&lt;/a&gt;, the model generates images in response to user text input in near real-time.&lt;/li&gt;
  &lt;li&gt;Although not perfectly real-time due to communication delays between the server and user and the time required to save images, the experience is significantly faster and more satisfying than before.
    &lt;ul&gt;
      &lt;li&gt;Feedback was collected from acquaintances to evaluate user satisfaction.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 01 Mar 2024 18:05:23 +0900</pubDate>
        <link>http://localhost:4000/proj/realtime/</link>
        <guid isPermaLink="true">http://localhost:4000/proj/realtime/</guid>
      </item>
    
      <item>
        <title>Robust-QA using DistilBERT</title>
        <description>&lt;div align=&quot;center&quot; style=&quot;
  font-family: &apos;Times New Roman&apos;, Times, serif;
  font-size: 20px;
  font-weight: bold;
  color: #4a4a4a;
  padding: 20px;
  margin: 20px auto;
  border: 2px solid #e0e0e0;
  border-radius: 10px;
  background: linear-gradient(120deg, #f0f8ff, #ffffff);
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);&quot;&gt;
  üöÄ &quot;Improving performance on out-of-domain datasets using 
  &lt;br /&gt; i) text data augmentation, ii) AutoML.&quot; üåü
&lt;/div&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The goal of this project is to build a robust Question Answering (QA) model that uses DistilBERT (a lightweight version of BERT) trained on In-domain datasets (SQuAD, NewsQA, Natural Questions) and performs well on unseen Out-domain datasets (DuoRC, RACE, RelationExtraction).
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Train Dataset&lt;/strong&gt;: $242,304$ (SQuAD, NewsQA, Natural Questions)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Validation Dataset&lt;/strong&gt;: $38,888$ (SQuAD, NewsQA, Natural Questions)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Test Dataset&lt;/strong&gt;: $721$ (DuoRC, RACE, RelationExtraction)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The evaluation metrics are F1 Score and Exact Match (EM).
    &lt;ul&gt;
      &lt;li&gt;Baseline (DistilBERT)
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;F1 score&lt;/strong&gt;: $48.41$&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;EM&lt;/strong&gt;: $31.94$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;problem&quot;&gt;Problem&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Developing a novel and groundbreaking methodology within a limited timeframe (1 month) is challenging.&lt;/li&gt;
  &lt;li&gt;Therefore, we focused on simple and intuitive approaches to improve performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;proposed-method&quot;&gt;Proposed Method&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data augmentation&lt;/strong&gt; (&lt;a href=&quot;fine-tuning&quot;&gt;EDA: Easy Data Augmentation&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;Augmenting text data is a challenging task.
        &lt;ul&gt;
          &lt;li&gt;EDA, initially proposed for classification tasks, maintains the data distribution while augmenting text.&lt;/li&gt;
          &lt;li&gt;However, it is not directly suitable for QA tasks, so we modified it as follows:
            &lt;ul&gt;
              &lt;li&gt;Preserve case sensitivity.&lt;/li&gt;
              &lt;li&gt;Keep answer words intact and modify only the remaining words.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;EDA Techniques:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Synonym Replacement (SR): Replace n randomly selected words (excluding stop words) with their synonyms.&lt;/li&gt;
    &lt;li&gt;Random Insertion (RI): Choose a synonym of a randomly selected word (excluding stop words) and insert it into a random position in the sentence.&lt;/li&gt;
    &lt;li&gt;Random Swap (RS): Randomly pick two words in the sentence and swap their positions.&lt;/li&gt;
    &lt;li&gt;Random Deletion (RD): Delete words in the sentence with a probability of $p$.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;AutoML&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;We explored various hyperparameters to optimize the model:
        &lt;ul&gt;
          &lt;li&gt;Lr_scheduler: None, LambdaLR, MultiplicativeLR, StepLR, CosineAnnealingWarmUpRestarts&lt;/li&gt;
          &lt;li&gt;Optimizer: None, RMSprop, Adam, AdamW, SGD&lt;/li&gt;
          &lt;li&gt;Learning rate: $[2e-5, 2e-4]$&lt;/li&gt;
          &lt;li&gt;Batch size: {$16, 32, 64$}&lt;/li&gt;
          &lt;li&gt;Epochs: $[2, 5]$&lt;/li&gt;
          &lt;li&gt;Train: last layer vs + last transformer block vs fine-tuning&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;EDA Results
    &lt;ul&gt;
      &lt;li&gt;An augmentation probability of $p = 0.1$ (10%) caused instability.&lt;/li&gt;
      &lt;li&gt;A lower probability, $p = 0.01$ (1%), contributed to more stable performance improvements.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;AutoML Results
    &lt;ul&gt;
      &lt;li&gt;Both SGD Optimizer and Partial Training consistently degraded performance, so they were excluded.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Final Results
    &lt;ul&gt;
      &lt;li&gt;As shown in the figure below, the performance improved significantly:
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;F1 Score&lt;/strong&gt;: $50.31$ (baseline: $48.41$)&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Exact Match&lt;/strong&gt;: $35.12$ (baseline: $31.94$)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 01 Mar 2024 18:05:23 +0900</pubDate>
        <link>http://localhost:4000/proj/robustqa/</link>
        <guid isPermaLink="true">http://localhost:4000/proj/robustqa/</guid>
      </item>
    
      <item>
        <title>StyleBoost: A Study of Personalizing Text-to-Image Generation in Any Style using DreamBooth</title>
        <description>&lt;div align=&quot;center&quot; style=&quot;
  font-family: &apos;Times New Roman&apos;, Times, serif;
  font-size: 20px;
  font-weight: bold;
  color: #4a4a4a;
  padding: 20px;
  margin: 20px auto;
  border: 2px solid #e0e0e0;
  border-radius: 10px;
  background: linear-gradient(120deg, #f0f8ff, #ffffff);
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);&quot;&gt;
  üöÄ &quot;Style Personalizing in Text-to-Image Synthesis &lt;br /&gt; using DreamBooth&quot; üåü
&lt;/div&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Recent advancements in text-to-image models, such as Stable Diffusion, have demonstrated their ability to synthesize visual images through natural language prompts.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.12242&quot;&gt;DreamBooth&lt;/a&gt; is a recent method for personalization of pre-trained text-to-image diffusion model, such as Stable Diffusion, using only a few images of a specific object, called instance images.
    &lt;ul&gt;
      &lt;li&gt;Using $3 ‚àí 5$ images of the specific object &lt;em&gt;(e.g., my dog)&lt;/em&gt; paired with a text prompt &lt;em&gt;(e.g., ‚ÄúA [V] dog‚Äù)&lt;/em&gt; consisting of a unique token identifier &lt;em&gt;(e.g., ‚Äú[V]‚Äù)&lt;/em&gt; representing the given object &lt;em&gt;(i.e., my dog)&lt;/em&gt; and the corresponding class name &lt;em&gt;(e.g., ‚Äúdog‚Äù)&lt;/em&gt;, DreamBooth fine-tunes a text-to-image diffusion model to encode the unique token with the subject.&lt;/li&gt;
      &lt;li&gt;To this end, DreamBooth introduces a class-specific prior preservation loss that encourages the fine-tuned model to keep semantic knowledge about the class prior &lt;em&gt;(i.e., ‚Äúdog‚Äù)&lt;/em&gt; and produce diverse instances of the class &lt;em&gt;(e.g., various dogs)&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Loss proposed by DreamBooth
    &lt;ul&gt;
      &lt;li&gt;$\mathbf{x}$: ground-truth image for text prompt &lt;em&gt;(e.g., ‚ÄúA [V] dog‚Äù)&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;$\mathbf{c}$: conditioning vector &lt;em&gt;(e.g., obtained from ‚ÄúA [V] dog‚Äù)&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;$\epsilon, \epsilon‚Äô$: Gaussian noise&lt;/li&gt;
      &lt;li&gt;$\mathbf{x}_{\text{pr}}$: ground-truth image for class prior &lt;em&gt;(e.g., ‚Äúdog‚Äù)&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;$\mathbf{c}_{\text{pr}}$: conditioning vector for class prior prompt &lt;em&gt;(e.g., obtained from ‚Äúdog‚Äù)&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;$\alpha_t, \alpha_t‚Äô, \sigma_t, \sigma_t‚Äô, w_t, w_t‚Äô$: terms that control noise schedule and sample quality&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/styleboost_2.png&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;problem&quot;&gt;Problem&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;DreamBooth‚Äôs personalization capabilities are excellent, especially for clear subjects like objects. However, learning to generate images that encapsulate various art styles remains a challenging problem due to the abstract and broad visual perceptions required for stylistic attributes such as lines, shapes, textures, and colors.&lt;/li&gt;
  &lt;li&gt;In this paper, we aim to inherit DreamBooth‚Äôs personalization abilities while effectively binding the abstract concept of &lt;strong&gt;‚Äúart style.‚Äù&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Instance prompt/images $\rightarrow$ StyleRef prompt/images&lt;/li&gt;
      &lt;li&gt;Class prior prompt/image $\rightarrow$ Aux prompt/images&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;proposed-method&quot;&gt;Proposed Method&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;We hypothesize that DreamBooth struggles to learn abstract concepts due to the following reasons:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;StyleRef prompt (e.g., ‚ÄúA [V] style‚Äù)&lt;/strong&gt; and &lt;strong&gt;Aux prompt (e.g., ‚Äústyle‚Äù)&lt;/strong&gt; share the same token, and as a result, they influence each other during the learning process.&lt;/li&gt;
      &lt;li&gt;During learning, to prevent the Aux prompt &lt;em&gt;(e.g., ‚Äústyle‚Äù)&lt;/em&gt; from being lost, the Aux prompt is provided to generate images, and the corresponding token is re-learned. However, the pre-trained model has learned the Aux prompt as referring to fashion style, which is not useful for learning the target style.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To address this, we propose the following method:
    &lt;ul&gt;
      &lt;li&gt;Aux images are carefully selected to represent art works and people that are related to the target style, rather than fashion styles (high-resolution images).
        &lt;ul&gt;
          &lt;li&gt;Detailed descriptions of people &lt;em&gt;(e.g., hands, legs, face, and full-body shots)&lt;/em&gt; remain important for qualitative evaluation, while landscapes or animal images are less sensitive. Therefore, Aux images mainly consist of portraits and/or images of people. This helps in generating high-quality images for people-related prompts.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Since learning style from only $3 - 5$ images is difficult, we proceed with training using around $15 - 20$ images.&lt;/li&gt;
      &lt;li&gt;Our approach uses around $15 - 20$ images for both StyleRef and Aux images, establishing a foundational binding of the unique token identifier with a broad range of the target style. The Aux images are carefully chosen to strengthen this binding. This dual-binding strategy helps capture the essential concept of art styles and accelerates the learning of the diverse attributes of the target style.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Aux images ablation for three target styles, displaying FID and CLIP scores.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/styleboost3.png&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Experimental evaluation on three styles‚Äîrealism art, SureB art, and anime‚Äîdemonstrates significant improvements in both the quality of generated images and perceptual fidelity metrics, such as FID and CLIP scores.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;‚ÄúStyleRef‚Äù&lt;/strong&gt;: The composition of the target image should appropriately mix people and backgrounds to comprehensively understand the target style.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;‚ÄúAux‚Äù&lt;/strong&gt;: The Aux images should be composed in a style that can assist the target style.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/styleboost_4.png&quot; /&gt;
&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Sep 2023 18:05:23 +0900</pubDate>
        <link>http://localhost:4000/blog/styleboost</link>
        <guid isPermaLink="true">http://localhost:4000/blog/styleboost</guid>
      </item>
    
      <item>
        <title>Reinforcement Learning Practice</title>
        <description>&lt;div align=&quot;center&quot; style=&quot;
  font-family: &apos;Times New Roman&apos;, Times, serif;
  font-size: 20px;
  font-weight: bold;
  color: #4a4a4a;
  padding: 20px;
  margin: 20px auto;
  border: 2px solid #e0e0e0;
  border-radius: 10px;
  background: linear-gradient(120deg, #f0f8ff, #ffffff);
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);&quot;&gt;
  üöÄ &quot;Implementation of Basic RL Algorithms&quot; üåü
&lt;/div&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h3 id=&quot;algorithm-test&quot;&gt;Algorithm Test&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Basic RL algorithms were implemented and tested in the OpenAI Gym environment.&lt;/li&gt;
  &lt;li&gt;The implemented algorithms are as follows:
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DQN, DDQN, Dueling DQN, DDQN + Dueling, PPO, A2C, A3C&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;uav-with-ddpg&quot;&gt;UAV with DDPG&lt;/h3&gt;

&lt;p&gt;$\textbf{Motivation}$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A paper presented an experiment where a UAV uses the received signal strength as a reward and employs reinforcement learning (DDPG algorithm) to approach the user.&lt;/li&gt;
  &lt;li&gt;The project was carried out with the idea of directly implementing the paper.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$\textbf{Environment}$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The UAV must reach the user.&lt;/li&gt;
  &lt;li&gt;The starting position of the UAV is fixed at ($20, 20, 200$).&lt;/li&gt;
  &lt;li&gt;K users (default: $5$) are generated at random positions between [&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rd.randint(150, 180), rd.randint(150, 180), 0&lt;/code&gt;].&lt;/li&gt;
  &lt;li&gt;The DDPG algorithm is used, and the reward is based on the received signal strength between the user and the UAV.&lt;/li&gt;
  &lt;li&gt;The network is implemented using a simple DNN.&lt;/li&gt;
  &lt;li&gt;The experiment ends after $60$ steps.&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&quot;display: flex; justify-content: center; gap: 20px;&quot;&gt;
  &lt;div style=&quot;flex: 1; text-align: center;&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/assets/img/posts/rl_3.png&quot; alt=&quot;Image 1&quot; /&gt;
    &lt;figcaption&gt;Caption 1: Before training&lt;/figcaption&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div style=&quot;flex: 1; text-align: center;&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/assets/img/posts/rl_5.png&quot; alt=&quot;Image 2&quot; /&gt;
    &lt;figcaption&gt;Caption 1: After training&lt;/figcaption&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;black-jack-with-td-vs-mc&quot;&gt;Black Jack with TD vs MC&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;TD/MC prediction/control&lt;/li&gt;
  &lt;li&gt;The results vary depending on the presence of ACE.
    &lt;ul&gt;
      &lt;li&gt;Areas marked with a star represent &lt;em&gt;‚Äúhit,‚Äù&lt;/em&gt; while the unmarked areas represent &lt;em&gt;‚Äústand.‚Äù&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&quot;display: flex; justify-content: center; gap: 20px;&quot;&gt;
  &lt;div style=&quot;flex: 1; text-align: center;&quot;&gt;
    &lt;figure&gt;
      &lt;img src=&quot;/assets/img/posts/rl_6.png&quot; alt=&quot;Image 1&quot; /&gt;
      &lt;figcaption&gt;Caption 1: Use ACE as 1&lt;/figcaption&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div style=&quot;flex: 1; text-align: center;&quot;&gt;
    &lt;figure&gt;
      &lt;img src=&quot;/assets/img/posts/rl_7.png&quot; alt=&quot;Image 2&quot; /&gt;
      &lt;figcaption&gt;Caption 2: Use ACE as 11&lt;/figcaption&gt;
    &lt;/figure&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;unity-with-ppo&quot;&gt;Unity with PPO&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Unity provides a service that allows various RL algorithms to be applied to environments created with Unity.&lt;/li&gt;
  &lt;li&gt;A car racing environment was created, and the PPO algorithm was applied to test whether it could complete the race.&lt;/li&gt;
  &lt;li&gt;The configuration is as follows:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;behaviors:
  ArcadeDriver:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024
      buffer_size: 204800
      learning_rate: 0.0003
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 7
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 256
      num_layers: 2
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.995
        strength: 1.0
    keep_checkpoints: 200
    max_steps: 100000000
    time_horizon: 32
    summary_freq: 10000
    checkpoint_interval: 50000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
  &lt;source src=&quot;/assets/img/posts/rl_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;

&lt;h3 id=&quot;dodge-the-poop-game&quot;&gt;Dodge the poop game&lt;/h3&gt;

&lt;p&gt;$\textbf{Environment}$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Action: Move left or right&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Termination: Terminated when hit by the ball&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reward: $-0.01$ over time, $+1$ for small ball, $+10$ for large ball&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Base network: MLP&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Algorithms tested: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DQN, A2C, DDPG, A3C, PPO, PPO + RNN, PPO + LSTM, DDQN&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Main result: The task was easy, resulting in good performance from DNN, DDQN, and DQN.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;style&gt;
  table {
    width: 100%;
    border-collapse: collapse;
    margin: 20px 0;
  }

  table, th, td {
    border: 1px solid #ddd; /* ÏÑ† ÏÉâÍπîÏùÑ Ïó∞Ìïú ÌöåÏÉâÏúºÎ°ú ÏÑ§Ï†ï */
  }

  th, td {
    padding: 12px 15px; /* ÏÖÄ Ïó¨Î∞±ÏùÑ ÎÑâÎÑâÌûà ÏÑ§Ï†ï */
    text-align: center; /* ÌÖçÏä§Ìä∏ Ï§ëÏïô Ï†ïÎ†¨ */
  }

  th {
    background-color: #f4f4f9; /* Ìó§Îçî Î∞∞Í≤ΩÏùÑ Ïó∞Ìïú ÌöåÏÉâÏúºÎ°ú ÏÑ§Ï†ï */
    font-weight: bold; /* Ìó§Îçî Í∏ÄÏî® ÎëêÍªçÍ≤å */
  }

  tr:nth-child(even) {
    background-color: #f9f9f9; /* ÏßùÏàò Î≤àÏß∏ ÌñâÏóê Î∞∞Í≤ΩÏÉâÏùÑ Îã¨Î¶¨ ÏÑ§Ï†ï */
  }

  tr:hover {
    background-color: #f1f1f1; /* ÎßàÏö∞Ïä§Î•º Ïò¨Î†∏ÏùÑ Îïå Ìñâ Î∞∞Í≤ΩÏÉâ Î≥ÄÍ≤Ω */
  }

  td {
    font-size: 14px; /* Ìëú ÎÇ¥Ïö© Í∏ÄÏûê ÌÅ¨Í∏∞ ÏÑ§Ï†ï */
  }

  strong {
    color: #2d9c2d; /* Í∞ïÏ°∞Îêú ÌÖçÏä§Ìä∏ ÏÉâÍπîÏùÑ Ï¥àÎ°ùÏÉâÏúºÎ°ú ÏÑ§Ï†ï */
  }
&lt;/style&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Method&lt;/th&gt;
      &lt;th&gt;# of layers&lt;/th&gt;
      &lt;th&gt;Accelerated epi&lt;/th&gt;
      &lt;th&gt;Best score&lt;/th&gt;
      &lt;th&gt;Epi achieved best score&lt;/th&gt;
      &lt;th&gt;Avg score ($\times 10^2$)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;DQN&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;400&lt;/td&gt;
      &lt;td&gt;1578&lt;/td&gt;
      &lt;td&gt;178&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DQN with CNN&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;x&lt;/td&gt;
      &lt;td&gt;x&lt;/td&gt;
      &lt;td&gt;x&lt;/td&gt;
      &lt;td&gt;x&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DDQN&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;400&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;1729&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;299&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PPO&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;600&lt;/td&gt;
      &lt;td&gt;173&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PPO with RNN&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;x&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PPO with LSTM&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5000&lt;/td&gt;
      &lt;td&gt;140&lt;/td&gt;
      &lt;td&gt;205&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Sat, 17 Jun 2023 18:05:23 +0900</pubDate>
        <link>http://localhost:4000/proj/rl/</link>
        <guid isPermaLink="true">http://localhost:4000/proj/rl/</guid>
      </item>
    
      <item>
        <title>Consistent Web Novel Illustration Generation Project</title>
        <description>&lt;div align=&quot;center&quot; style=&quot;
  font-family: &apos;Times New Roman&apos;, Times, serif;
  font-size: 20px;
  font-weight: bold;
  color: #4a4a4a;
  padding: 20px;
  margin: 20px auto;
  border: 2px solid #e0e0e0;
  border-radius: 10px;
  background: linear-gradient(120deg, #f0f8ff, #ffffff);
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);&quot;&gt;
  üöÄ &quot;Provides Consistent Text-to-Image Synthesis for Characters in Web Novel&quot; üåü
&lt;/div&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Web novels often include illustrations of characters, either on the cover or within the text. These illustrations are typically outsourced to artists to ensure consistent character designs.&lt;/li&gt;
  &lt;li&gt;However, communication issues between the author and the illustrator can lead to several challenges:
    &lt;ul&gt;
      &lt;li&gt;Time and cost inefficiencies&lt;/li&gt;
      &lt;li&gt;Failure to capture the character as envisioned by the author&lt;/li&gt;
      &lt;li&gt;Lack of immediate reflection of the author‚Äôs feedback&lt;/li&gt;
      &lt;li&gt;Limited to the illustrator‚Äôs particular drawing style&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To address these issues, this project aims to build an AI service that provides consistent character illustrations for web novel platforms, establishing a stable system to solve the above problems.
    &lt;ul&gt;
      &lt;li&gt;The image generation model used is the &lt;strong&gt;Latent Diffusion Model (LDM)&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;In this project, &lt;em&gt;the author (Junseo Park)&lt;/em&gt; focused on developing the AI model and server infrastructure.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;problem&quot;&gt;Problem&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;LDM generates entirely different characters even with minor text changes, despite having a fixed seed value.&lt;/li&gt;
  &lt;li&gt;A personalization methodology is required to create characters that remain consistent despite variations in text input.&lt;/li&gt;
  &lt;li&gt;Assigning a separate model for each character is cost-inefficient. A single model must be capable of generating all characters within the same style.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;proposed-method&quot;&gt;Proposed Method&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The generation process is as follows:
    &lt;ol&gt;
      &lt;li&gt;First, the author selects a model (RevAnimate, DreamShaper, Pastelboys, Counterfeit) that can generate images in two styles (mid-journey or anime) to match the story‚Äôs characters.&lt;/li&gt;
      &lt;li&gt;After selecting tags corresponding to the character, the author generates an image.&lt;/li&gt;
      &lt;li&gt;Once the desired character is generated, the author selects the character.&lt;/li&gt;
      &lt;li&gt;Since a single image is insufficient for training, $20$ images with various backgrounds, expressions, and angles are generated based on a prompt template.
        &lt;ul&gt;
          &lt;li&gt;To maintain character consistency, the original prompt is preserved with slight modifications:&lt;/li&gt;
        &lt;/ul&gt;
        &lt;ul&gt;
          &lt;li&gt;E.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{original prompt}, ((upper body)), in the mountain, looking at viewer&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The $20$ images are used to train the model using DreamBooth, a process that takes about $30$ minutes.&lt;/li&gt;
      &lt;li&gt;The author can then generate consistent character illustrations as needed.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/webnovel2.png&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To provide consistent and personalized characters, &lt;a href=&quot;https://arxiv.org/abs/2208.12242&quot;&gt;DreamBooth&lt;/a&gt; is utilized:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;DreamBooth&lt;/strong&gt; is a personalization methodology that binds a pseudo word to a subject. Each character is mapped to a unique pseudo word (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zxw&lt;/code&gt;), allowing multiple characters to be bound to a single model.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The proposed service offers the following benefits:
    &lt;ul&gt;
      &lt;li&gt;Resolves time and cost inefficiencies&lt;/li&gt;
      &lt;li&gt;Generates characters as envisioned by the author&lt;/li&gt;
      &lt;li&gt;Simplifies the process with convenient tag-based generation&lt;/li&gt;
      &lt;li&gt;Immediately reflects the author‚Äôs feedback&lt;/li&gt;
      &lt;li&gt;Enables image generation within ~$30$ seconds&lt;/li&gt;
      &lt;li&gt;Offers diverse image styles&lt;/li&gt;
      &lt;li&gt;Provides consistent character illustrations&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;By integrating a stable AI service into the web novel platform, authors can now generate illustrations more flexibly and conveniently.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/webnovel3.png&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/posts/webnovel4.png&quot; /&gt;
&lt;/p&gt;
</description>
        <pubDate>Wed, 01 Mar 2023 18:05:23 +0900</pubDate>
        <link>http://localhost:4000/proj/consistent-web-novel/</link>
        <guid isPermaLink="true">http://localhost:4000/proj/consistent-web-novel/</guid>
      </item>
    

    
      
        
      
    
      
    
      
        
          <item>
            <title></title>
            <description>&lt;h3&gt;   &lt;/h3&gt;

&lt;div id=&quot;categories&quot;&gt;

  &lt;div class=&quot;category-box&quot;&gt;
    
    &lt;div id=&quot;#project&quot;&gt;&lt;/div&gt;
    &lt;h4 class=&quot;category-head&quot;&gt;&lt;a href=&quot;/blog/categories/project&quot;&gt;project&lt;/a&gt;&lt;/h4&gt;
    &lt;a name=&quot;project&quot;&gt;&lt;/a&gt;
     
    &lt;article class=&quot;center&quot;&gt;
      &lt;h6&gt;&lt;a href=&quot;/proj/realtime/&quot;&gt;Real-Time Text-to-Image synthesis with LCM-LoRA&lt;/a&gt;&lt;/h6&gt;
    &lt;/article&gt;


    
    &lt;article class=&quot;center&quot;&gt;
      &lt;h6&gt;&lt;a href=&quot;/proj/robustqa/&quot;&gt;Robust-QA using DistilBERT&lt;/a&gt;&lt;/h6&gt;
    &lt;/article&gt;


    
    &lt;article class=&quot;center&quot;&gt;
      &lt;h6&gt;&lt;a href=&quot;/proj/rl/&quot;&gt;Reinforcement Learning Practice&lt;/a&gt;&lt;/h6&gt;
    &lt;/article&gt;


    
    &lt;article class=&quot;center&quot;&gt;
      &lt;h6&gt;&lt;a href=&quot;/proj/consistent-web-novel/&quot;&gt;Consistent Web Novel Illustration Generation Project&lt;/a&gt;&lt;/h6&gt;
    &lt;/article&gt;


    

  &lt;/div&gt;

  &lt;div class=&quot;category-box&quot;&gt;
    
    &lt;div id=&quot;#conference&quot;&gt;&lt;/div&gt;
    &lt;h4 class=&quot;category-head&quot;&gt;&lt;a href=&quot;/blog/categories/conference&quot;&gt;conference&lt;/a&gt;&lt;/h4&gt;
    &lt;a name=&quot;conference&quot;&gt;&lt;/a&gt;
     
    &lt;article class=&quot;center&quot;&gt;
      &lt;h6&gt;&lt;a href=&quot;/blog/i2am/&quot;&gt;I2AM: Interpreting Image-to-Image Latent Diffusion Models via Bi-Attribution Maps&lt;/a&gt;&lt;/h6&gt;
    &lt;/article&gt;


    
    &lt;article class=&quot;center&quot;&gt;
      &lt;h6&gt;&lt;a href=&quot;/blog/styleboost&quot;&gt;StyleBoost: A Study of Personalizing Text-to-Image Generation in Any Style using DreamBooth&lt;/a&gt;&lt;/h6&gt;
    &lt;/article&gt;


    

  &lt;/div&gt;

  &lt;div class=&quot;category-box&quot;&gt;
    
    &lt;div id=&quot;#journal&quot;&gt;&lt;/div&gt;
    &lt;h4 class=&quot;category-head&quot;&gt;&lt;a href=&quot;/blog/categories/journal&quot;&gt;journal&lt;/a&gt;&lt;/h4&gt;
    &lt;a name=&quot;journal&quot;&gt;&lt;/a&gt;
     
    &lt;article class=&quot;center&quot;&gt;
      &lt;h6&gt;&lt;a href=&quot;/blog/styleforge/&quot;&gt;StyleForge: Enhancing Text-to-Image Synthesis for Any Artistic Styles with Dual Binding&lt;/a&gt;&lt;/h6&gt;
    &lt;/article&gt;


    

  &lt;/div&gt;

&lt;/div&gt;

</description>
            <link>http://localhost:4000/blog/categories/</link>
          </item>
        
      
    
      
    
      
    
      
        
          <item>
            <title>Guides</title>
            <description>&lt;h5&gt; Posts by Category : {{ page.title }} &lt;/h5&gt;

&lt;div class=&quot;card&quot;&gt;
{% for post in site.categories.guides %}
 &lt;li class=&quot;category-posts&quot;&gt;&lt;span&gt;{{ post.date | date_to_string }}&lt;/span&gt; &amp;nbsp; &lt;a href=&quot;{{ post.url }}&quot;&gt;{{ post.title }}&lt;/a&gt;&lt;/li&gt;
{% endfor %}
&lt;/div&gt;</description>
            <link>http://localhost:4000/blog/categories/guides/</link>
          </item>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <item>
            <title>Get Started</title>
            <description>## Getting Started - How to use ‚Äúdevlopr-jekyll‚Äù theme

## What&apos;s Jekyll ?

If you aren‚Äôt familiar with Jekyll yet, you should know that it is a static site generator. It will transform your plain text into static websites and blogs. No more databases, slow loading websites, risk of being hacked‚Ä¶just your content. And not only that, with Jekyll you get free hosting with GitHub Pages! If you are a beginner we recommend you start with [Jekyll‚Äôs Docs](https://jekyllrb.com/docs/installation/). Now, if you know how to use Jekyll, let‚Äôs move on to using this theme in Jekyll:

## Watch Tutorial

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/cXBEfpn0qrg?rel=0&amp;amp;controls=0&amp;amp;showinfo=0&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;


### Steps to create your blog using devlopr-jekyll and Host using Github Pages :

&gt;  **Step 1.**  Fork the repo - [click here](https://github.com/sujaykundu777/devlopr-jekyll/fork)

![Devlopr Jekyll Repo](/assets/img/posts/fork1.PNG){:class=&quot;img-fluid&quot;}

&gt; **Step 2.** Use **your-github-username.github.io** as the new repo  ( Replace your-github-username with yours). Remember if you use the name other than your-github-username.github.io , your blog will be built using gh-pages branch.

![Devlopr Jekyll Repo](/assets/img/posts/fork2.PNG){:class=&quot;img-fluid&quot;}

![Devlopr Jekyll Repo](/assets/img/posts/fork3.PNG){:class=&quot;img-fluid&quot;}

&gt; **Step 3.** Clone the new repo locally to make changes :

![Devlopr Jekyll Repo](/assets/img/posts/fork31.PNG){:class=&quot;img-fluid&quot;}

![Devlopr Jekyll Repo](/assets/img/posts/fork32.PNG){:class=&quot;img-fluid&quot;}

![Devlopr Jekyll Repo](/assets/img/posts/fork33.PNG){:class=&quot;img-fluid&quot;}

```bash
 $ git clone https://github.com/yourusername/yourusername.github.io
 $ cd yourusername.github.io
 $ code .
```

&gt; **Step 4.** Open the files using VSCode and edit _config.yml and edit with your details:

- _config.yml file - replace with your own details
- _posts - Add your blog posts here
- _includes - You can replace the contents of the files with your data. (contains widgets)
- _assets/img - Add all your images here

![Devlopr Jekyll Repo](/assets/img/posts/fork34.PNG){:class=&quot;img-fluid&quot;}

&gt; **Step 5** - Install the development requirements:

### Set up local development environment

1. [Git](https://git-scm.com/)
2. [Ruby](https://www.ruby-lang.org/) and [Bundler](https://bundler.io/)
3. [VSCode](https://code.visualstudio.com/download)

We need ruby and bundler to build our site locally. After installation check if its working:

For ruby :

```bash
$ ruby -v
ruby 2.5.1p57 (2018-03-29 revision 63029) [x86_64-linux-gnu]
```
For bundler :

```bash
$ gem install bundler
$ bundler -v
Bundler version 2.2.29
```
Add jekyll :

```bash
$ bundle update
$ bundle add jekyll
```
 This command will add the Jekyll gem to our Gemfile and install it to the ./vendor/bundle/ folder.

You can check the jekyll version

```
$ bundle exec jekyll -v
jekyll 4.2.0
```

&gt; **Step 6.** Install the gem dependencies by running the following command

```bash
$ bundle update
$ bundle install
```

&gt; **Step 7.** Serve the site locally by running the following command below:

```bash
$ bundle exec jekyll serve --watch
```
or you can also serve using :

```bash
$ jekyll serve
```

Visit [http://localhost:4000](http://localhost:4000) for development server

![Devlopr Jekyll Repo](/assets/img/posts/fork41.PNG){:class=&quot;img-fluid&quot;}


### Adding Content

Start populating your blog by adding your .md files in _posts. devlopr-jekyll already has a few examples.

#### YAML Post Example:

```yml
---
layout: post
title: Sample Post
author: Sujay Kundu
date: &apos;2019-05-21 14:35:23 +0530&apos;
category:
        - jekyll
summary: This is the summary for the sample post
thumbnail: sample.png
---

Hi ! This is sample post.

```

#### YAML Page Example:

```yml
---
layout: page
title: Sample Page
permalink: /sample-page/
---

Hi ! This is sample page.
```

#### Editing stylesheet

You‚Äôll only work with a single file to edit/add theme style: assets/css/main.scss.

### Deploy your Changes

Once happy with your blog changes. Push your changes to master branch.

&gt; **Step 8.** Push Your Local Changes

```bash
 $ git add .
 $ git commit -m &quot;my new blog using devlopr-jekyll&quot;
 $ git push origin master
```

Visit your Github Repo settings ! Enable master branch as Github Pages Branch :

![Devlopr Jekyll Repo](/assets/img/posts/fork6.PNG){:class=&quot;img-fluid&quot;}

&gt; **Step 9.** Deploy your Blog :

![Devlopr Jekyll Repo](/assets/img/posts/fork7.PNG){:class=&quot;img-fluid&quot;}

&gt; Congrats ! On your new shining Blog !

You can visit the blog using [http://your-github-username.github.io](http://your-github-username.github.io).

</description>
            <link>http://localhost:4000/get-started/</link>
          </item>
        
      
    
      
        
          <item>
            <title>Jekyll</title>
            <description>&lt;h5&gt; Posts by Category : {{ page.title }} &lt;/h5&gt;

&lt;div class=&quot;card&quot;&gt;
{% for post in site.categories.jekyll %}
 &lt;li class=&quot;category-posts&quot;&gt;&lt;span&gt;{{ post.date | date_to_string }}&lt;/span&gt; &amp;nbsp; &lt;a href=&quot;{{ post.url }}&quot;&gt;{{ post.title }}&lt;/a&gt;&lt;/li&gt;
{% endfor %}
&lt;/div&gt;</description>
            <link>http://localhost:4000/blog/categories/jekyll/</link>
          </item>
        
      
    
      
    
      
    
      
        
          <item>
            <title>Guides</title>
            <description>&lt;h5&gt; Posts by Category : {{ page.title }} &lt;/h5&gt;

&lt;div class=&quot;card&quot;&gt;
{% for post in site.categories.sample_category %}
 &lt;li class=&quot;category-posts&quot;&gt;&lt;span&gt;{{ post.date | date_to_string }}&lt;/span&gt; &amp;nbsp; &lt;a href=&quot;{{ post.url }}&quot;&gt;{{ post.title }}&lt;/a&gt;&lt;/li&gt;
{% endfor %}
&lt;/div&gt;</description>
            <link>http://localhost:4000/blog/categories/sample_category/</link>
          </item>
        
      
    
      
    
      
        
          <item>
            <title>Our Sponsors</title>
            <description>Thanks to all the amazing contributors and our Backers for the support.

- [Dirish Mohan](https://dirishmohan.com)</description>
            <link>http://localhost:4000/sponsors/</link>
          </item>
        
      
    
      
        
          <item>
            <title>Styleguide</title>
            <description>### devlopr - Styleguide

&lt;hr /&gt;

 &lt;img src=&quot;/assets/img/styleguide.png&quot; class=&quot;img-fluid&quot;&gt;

&lt;p&gt; Lets try the different text styles  &lt;b&gt; Bold &lt;/b&gt; , &lt;strong&gt; Strong &lt;/strong&gt;, &lt;em&gt; Emphasis &lt;/em&gt;, &lt;i&gt; Italic &lt;/i&gt; &lt;/p&gt;


&lt;p&gt; Now, lets try different heading styles : &lt;/p&gt;

&lt;h1&gt; Hello in h1 ! &lt;/h1&gt;
&lt;h2&gt; Hello in h2 ! &lt;/h2&gt;
&lt;h3&gt; Hello in h3 ! &lt;/h3&gt;
&lt;h4&gt; Hello in h4 ! &lt;/h4&gt;
&lt;h5&gt; Hello in h5 ! &lt;/h5&gt;
&lt;h6&gt; Hello in h6 ! &lt;/h6&gt;

&lt;hr /&gt;
&lt;p&gt; Unordered List &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; List Item 1 &lt;/li&gt;
&lt;li&gt; List Item 2 &lt;/li&gt;
&lt;li&gt; List Item 3 &lt;/li&gt;
&lt;li&gt; List Item 4 &lt;/li&gt;
&lt;li&gt; List Item 5 &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; Ordered List &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt; List Item 1 &lt;/li&gt;
&lt;li&gt; List Item 2 &lt;/li&gt;
&lt;li&gt; List Item 3 &lt;/li&gt;
&lt;li&gt; List Item 4 &lt;/li&gt;
&lt;li&gt; List Item 5 &lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;This is a Block Quote,  It can Expand Multiple Lines &lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;You can use the mark tag to &lt;mark&gt;highlight&lt;/mark&gt; text. &lt;/p&gt;

&lt;p&gt;&lt;del&gt; This line of text is meant to be deleted text &lt;/del&gt; &lt;/p&gt;

&lt;p&gt;&lt;u&gt;This line of text will render as underlined&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;This line of text is meant to be treated as fine print.&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This line rendered as bold text.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This line rendered as italicized text.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;abbr title=&quot;attribute&quot;&gt;attr&lt;/abbr&gt;&lt;/p&gt;
&lt;p&gt;&lt;abbr title=&quot;HyperText Markup Language&quot; class=&quot;initialism&quot;&gt;HTML&lt;/abbr&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;responsive-table&quot;&gt;
&lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th scope=&quot;col&quot;&gt;#&lt;/th&gt;
          &lt;th scope=&quot;col&quot;&gt;Heading&lt;/th&gt;
          &lt;th scope=&quot;col&quot;&gt;Heading&lt;/th&gt;
          &lt;th scope=&quot;col&quot;&gt;Heading&lt;/th&gt;
          &lt;th scope=&quot;col&quot;&gt;Heading&lt;/th&gt;
          &lt;th scope=&quot;col&quot;&gt;Heading&lt;/th&gt;
          &lt;th scope=&quot;col&quot;&gt;Heading&lt;/th&gt;
          &lt;th scope=&quot;col&quot;&gt;Heading&lt;/th&gt;
          &lt;th scope=&quot;col&quot;&gt;Heading&lt;/th&gt;
          &lt;th scope=&quot;col&quot;&gt;Heading&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;th scope=&quot;row&quot;&gt;1&lt;/th&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;th scope=&quot;row&quot;&gt;2&lt;/th&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;th scope=&quot;row&quot;&gt;3&lt;/th&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
          &lt;td&gt;Cell&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
    &lt;/div&gt;

&lt;hr /&gt;

&lt;h3&gt; Instagram Embed &lt;/h3&gt;

&lt;blockquote class=&quot;instagram-media&quot; data-instgrm-captioned data-instgrm-permalink=&quot;https://www.instagram.com/p/CBXO7AypXkM/?utm_source=ig_embed&amp;amp;utm_campaign=loading&quot; data-instgrm-version=&quot;13&quot; style=&quot; background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 1px; max-width:540px; min-width:326px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);&quot;&gt;&lt;div style=&quot;padding:16px;&quot;&gt; &lt;a href=&quot;https://www.instagram.com/p/CBXO7AypXkM/?utm_source=ig_embed&amp;amp;utm_campaign=loading&quot; style=&quot; background:#FFFFFF; line-height:0; padding:0 0; text-align:center; text-decoration:none; width:100%;&quot; target=&quot;_blank&quot;&gt; &lt;div style=&quot; display: flex; flex-direction: row; align-items: center;&quot;&gt; &lt;div style=&quot;background-color: #F4F4F4; border-radius: 50%; flex-grow: 0; height: 40px; margin-right: 14px; width: 40px;&quot;&gt;&lt;/div&gt; &lt;div style=&quot;display: flex; flex-direction: column; flex-grow: 1; justify-content: center;&quot;&gt; &lt;div style=&quot; background-color: #F4F4F4; border-radius: 4px; flex-grow: 0; height: 14px; margin-bottom: 6px; width: 100px;&quot;&gt;&lt;/div&gt; &lt;div style=&quot; background-color: #F4F4F4; border-radius: 4px; flex-grow: 0; height: 14px; width: 60px;&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&quot;padding: 19% 0;&quot;&gt;&lt;/div&gt; &lt;div style=&quot;display:block; height:50px; margin:0 auto 12px; width:50px;&quot;&gt;&lt;svg width=&quot;50px&quot; height=&quot;50px&quot; viewBox=&quot;0 0 60 60&quot; version=&quot;1.1&quot; xmlns=&quot;https://www.w3.org/2000/svg&quot; xmlns:xlink=&quot;https://www.w3.org/1999/xlink&quot;&gt;&lt;g stroke=&quot;none&quot; stroke-width=&quot;1&quot; fill=&quot;none&quot; fill-rule=&quot;evenodd&quot;&gt;&lt;g transform=&quot;translate(-511.000000, -20.000000)&quot; fill=&quot;#000000&quot;&gt;&lt;g&gt;&lt;path d=&quot;M556.869,30.41 C554.814,30.41 553.148,32.076 553.148,34.131 C553.148,36.186 554.814,37.852 556.869,37.852 C558.924,37.852 560.59,36.186 560.59,34.131 C560.59,32.076 558.924,30.41 556.869,30.41 M541,60.657 C535.114,60.657 530.342,55.887 530.342,50 C530.342,44.114 535.114,39.342 541,39.342 C546.887,39.342 551.658,44.114 551.658,50 C551.658,55.887 546.887,60.657 541,60.657 M541,33.886 C532.1,33.886 524.886,41.1 524.886,50 C524.886,58.899 532.1,66.113 541,66.113 C549.9,66.113 557.115,58.899 557.115,50 C557.115,41.1 549.9,33.886 541,33.886 M565.378,62.101 C565.244,65.022 564.756,66.606 564.346,67.663 C563.803,69.06 563.154,70.057 562.106,71.106 C561.058,72.155 560.06,72.803 558.662,73.347 C557.607,73.757 556.021,74.244 553.102,74.378 C549.944,74.521 548.997,74.552 541,74.552 C533.003,74.552 532.056,74.521 528.898,74.378 C525.979,74.244 524.393,73.757 523.338,73.347 C521.94,72.803 520.942,72.155 519.894,71.106 C518.846,70.057 518.197,69.06 517.654,67.663 C517.244,66.606 516.755,65.022 516.623,62.101 C516.479,58.943 516.448,57.996 516.448,50 C516.448,42.003 516.479,41.056 516.623,37.899 C516.755,34.978 517.244,33.391 517.654,32.338 C518.197,30.938 518.846,29.942 519.894,28.894 C520.942,27.846 521.94,27.196 523.338,26.654 C524.393,26.244 525.979,25.756 528.898,25.623 C532.057,25.479 533.004,25.448 541,25.448 C548.997,25.448 549.943,25.479 553.102,25.623 C556.021,25.756 557.607,26.244 558.662,26.654 C560.06,27.196 561.058,27.846 562.106,28.894 C563.154,29.942 563.803,30.938 564.346,32.338 C564.756,33.391 565.244,34.978 565.378,37.899 C565.522,41.056 565.552,42.003 565.552,50 C565.552,57.996 565.522,58.943 565.378,62.101 M570.82,37.631 C570.674,34.438 570.167,32.258 569.425,30.349 C568.659,28.377 567.633,26.702 565.965,25.035 C564.297,23.368 562.623,22.342 560.652,21.575 C558.743,20.834 556.562,20.326 553.369,20.18 C550.169,20.033 549.148,20 541,20 C532.853,20 531.831,20.033 528.631,20.18 C525.438,20.326 523.257,20.834 521.349,21.575 C519.376,22.342 517.703,23.368 516.035,25.035 C514.368,26.702 513.342,28.377 512.574,30.349 C511.834,32.258 511.326,34.438 511.181,37.631 C511.035,40.831 511,41.851 511,50 C511,58.147 511.035,59.17 511.181,62.369 C511.326,65.562 511.834,67.743 512.574,69.651 C513.342,71.625 514.368,73.296 516.035,74.965 C517.703,76.634 519.376,77.658 521.349,78.425 C523.257,79.167 525.438,79.673 528.631,79.82 C531.831,79.965 532.853,80.001 541,80.001 C549.148,80.001 550.169,79.965 553.369,79.82 C556.562,79.673 558.743,79.167 560.652,78.425 C562.623,77.658 564.297,76.634 565.965,74.965 C567.633,73.296 568.659,71.625 569.425,69.651 C570.167,67.743 570.674,65.562 570.82,62.369 C570.966,59.17 571,58.147 571,50 C571,41.851 570.966,40.831 570.82,37.631&quot;&gt;&lt;/path&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/div&gt;&lt;div style=&quot;padding-top: 8px;&quot;&gt; &lt;div style=&quot; color:#3897f0; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:550; line-height:18px;&quot;&gt; View this post on Instagram&lt;/div&gt;&lt;/div&gt;&lt;div style=&quot;padding: 12.5% 0;&quot;&gt;&lt;/div&gt; &lt;div style=&quot;display: flex; flex-direction: row; margin-bottom: 14px; align-items: center;&quot;&gt;&lt;div&gt; &lt;div style=&quot;background-color: #F4F4F4; border-radius: 50%; height: 12.5px; width: 12.5px; transform: translateX(0px) translateY(7px);&quot;&gt;&lt;/div&gt; &lt;div style=&quot;background-color: #F4F4F4; height: 12.5px; transform: rotate(-45deg) translateX(3px) translateY(1px); width: 12.5px; flex-grow: 0; margin-right: 14px; margin-left: 2px;&quot;&gt;&lt;/div&gt; &lt;div style=&quot;background-color: #F4F4F4; border-radius: 50%; height: 12.5px; width: 12.5px; transform: translateX(9px) translateY(-18px);&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&quot;margin-left: 8px;&quot;&gt; &lt;div style=&quot; background-color: #F4F4F4; border-radius: 50%; flex-grow: 0; height: 20px; width: 20px;&quot;&gt;&lt;/div&gt; &lt;div style=&quot; width: 0; height: 0; border-top: 2px solid transparent; border-left: 6px solid #f4f4f4; border-bottom: 2px solid transparent; transform: translateX(16px) translateY(-4px) rotate(30deg)&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&quot;margin-left: auto;&quot;&gt; &lt;div style=&quot; width: 0px; border-top: 8px solid #F4F4F4; border-right: 8px solid transparent; transform: translateY(16px);&quot;&gt;&lt;/div&gt; &lt;div style=&quot; background-color: #F4F4F4; flex-grow: 0; height: 12px; width: 16px; transform: translateY(-4px);&quot;&gt;&lt;/div&gt; &lt;div style=&quot; width: 0; height: 0; border-top: 8px solid #F4F4F4; border-left: 8px solid transparent; transform: translateY(-4px) translateX(8px);&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt; &lt;div style=&quot;display: flex; flex-direction: column; flex-grow: 1; justify-content: center; margin-bottom: 24px;&quot;&gt; &lt;div style=&quot; background-color: #F4F4F4; border-radius: 4px; flex-grow: 0; height: 14px; margin-bottom: 6px; width: 224px;&quot;&gt;&lt;/div&gt; &lt;div style=&quot; background-color: #F4F4F4; border-radius: 4px; flex-grow: 0; height: 14px; width: 144px;&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;p style=&quot; color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;&quot;&gt;&lt;a href=&quot;https://www.instagram.com/p/CBXO7AypXkM/?utm_source=ig_embed&amp;amp;utm_campaign=loading&quot; style=&quot; color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none;&quot; target=&quot;_blank&quot;&gt;A post shared by Sujay (@sujaykundu777)&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/blockquote&gt; &lt;script async src=&quot;//www.instagram.com/embed.js&quot;&gt;&lt;/script&gt;

&lt;hr&gt;

&lt;h3&gt; Twitter Embed &lt;/h3&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;I just published ‚ÄúDeploying a blog using Jekyll and Github Pages with SSL certificate for Free‚Äù &lt;a href=&quot;https://t.co/B3T3IQVU93&quot;&gt;https://t.co/B3T3IQVU93&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sujay Kundu (@SujayKundu777) &lt;a href=&quot;https://twitter.com/SujayKundu777/status/1012601950469160962?ref_src=twsrc%5Etfw&quot;&gt;June 29, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;hr /&gt;


&lt;h3&gt;YouTube Responsive Embed&lt;/h3&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/bBpKMH3nBzE?rel=0&amp;amp;controls=0&amp;amp;showinfo=0&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;h3&gt;Vimeo Responsive Embed&lt;/h3&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/212114694?title=0&amp;amp;byline=0&amp;amp;portrait=0&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;ted-responsive-embed&quot;&gt;TED Responsive Embed&lt;/h3&gt;

&lt;iframe src=&quot;https://embed.ted.com/talks/ted_halstead_a_climate_solution_where_all_sides_can_win&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;twitch-responsive-embed&quot;&gt;Twitch Responsive Embed&lt;/h3&gt;

&lt;iframe src=&quot;https://player.twitch.tv/?autoplay=false&amp;amp;video=v248755437&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; scrolling=&quot;no&quot; height=&quot;378&quot; width=&quot;620&quot;&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;soundcloud-embed&quot;&gt;SoundCloud Embed&lt;/h3&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;166&quot; scrolling=&quot;no&quot; frameborder=&quot;no&quot; src=&quot;https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/29738591&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&quot;&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;codepen-embed&quot;&gt;CodePen Embed&lt;/h3&gt;

&lt;p data-height=&quot;265&quot; data-theme-id=&quot;light&quot; data-slug-hash=&quot;YWvpRo&quot; data-default-tab=&quot;css,result&quot; data-user=&quot;kharrop&quot; data-embed-version=&quot;2&quot; data-pen-title=&quot;Referral Form&quot; class=&quot;codepen&quot;&gt;&lt;/p&gt;
&lt;script async=&quot;&quot; src=&quot;https://production-assets.codepen.io/assets/embed/ei.js&quot;&gt;&lt;/script&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;syntax-highlighting&quot;&gt;Syntax Highlighting&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;s1&quot;&gt;&apos;use strict&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;markdown&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;markdown&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;markdown&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Editor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;preview&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;update&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;preview&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;innerHTML&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;markdown&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;toHTML&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;editor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You can add inline code just like this, E.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;.code { color: #fff; }&lt;/code&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-css&quot; data-lang=&quot;css&quot;&gt;&lt;span class=&quot;nt&quot;&gt;pre&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;background-color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;#f4f4f4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;max-width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;overflow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;github-gist-embed&quot;&gt;GitHub gist Embed&lt;/h3&gt;

&lt;script src=&quot;https://gist.github.com/ahmadajmi/dbb4f713317721668bcbc39420562afc.js&quot;&gt;&lt;/script&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;input-style&quot;&gt;Input Style&lt;/h3&gt;

&lt;p&gt;&lt;input type=&quot;text&quot; placeholder=&quot;I&apos;m an input field!&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;


</description>
            <link>http://localhost:4000/styleguide/</link>
          </item>
        
      
    
      
    
      
    
      
    

  </channel>
</rss>