---
layout: post
title:  "Robust-QA using DistilBERT"
summary: "Make a robust QA model on out-of-domain"
author: Junseo Park
contributors: "<span style='font-weight: bold; color: #ff5733;'>Junseo Park</span>"
date: '2024-03-01 14:35:23 +0530'
category: project
thumbnail: /assets/img/posts/robustqa_1.png
keywords: large language model, DistilBERT, QA task, out-of-domain dataset
permalink: /proj/robustqa/
usemathjax: true
github_link: "https://github.com/qkrwnstj306/Robust-QA"
image: "/assets/img/posts/robustqa_1.png"
---
<div align="center" style="
  font-family: 'Times New Roman', Times, serif;
  font-size: 20px;
  font-weight: bold;
  color: #4a4a4a;
  padding: 20px;
  margin: 20px auto;
  border: 2px solid #e0e0e0;
  border-radius: 10px;
  background: linear-gradient(120deg, #f0f8ff, #ffffff);
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);">
  ðŸš€ "Improving performance on out-of-domain datasets using 
  <br> i) text data augmentation, ii) AutoML." ðŸŒŸ
</div>

  
<p></p>


### Context

- The goal of this project is to build a robust Question Answering (QA) model that uses DistilBERT (a lightweight version of BERT) trained on In-domain datasets (SQuAD, NewsQA, Natural Questions) and performs well on unseen Out-domain datasets (DuoRC, RACE, RelationExtraction).
  - **Train Dataset**: $242,304$ (SQuAD, NewsQA, Natural Questions)
  - **Validation Dataset**: $38,888$ (SQuAD, NewsQA, Natural Questions)
  - **Test Dataset**: $721$ (DuoRC, RACE, RelationExtraction)
- The evaluation metrics are F1 Score and Exact Match (EM).
  - Baseline (DistilBERT)
    - **F1 score**: $48.41$
    - **EM**: $31.94$

### Problem

- Developing a novel and groundbreaking methodology within a limited timeframe (1 month) is challenging.
- Therefore, we focused on simple and intuitive approaches to improve performance.

### Proposed Method

- **Data augmentation** (<a href='fine-tuning'>EDA: Easy Data Augmentation</a>)
  - Augmenting text data is a challenging task. 
    - EDA, initially proposed for classification tasks, maintains the data distribution while augmenting text.
    - However, it is not directly suitable for QA tasks, so we modified it as follows:
      - Preserve case sensitivity.
      - Keep answer words intact and modify only the remaining words.


> EDA Techniques:
> - Synonym Replacement (SR): Replace n randomly selected words (excluding stop words) with their synonyms.
> - Random Insertion (RI): Choose a synonym of a randomly selected word (excluding stop words) and insert it into a random position in the sentence. 
> - Random Swap (RS): Randomly pick two words in the sentence and swap their positions.
> - Random Deletion (RD): Delete words in the sentence with a probability of $p$.


- **AutoML**
  - We explored various hyperparameters to optimize the model:
    - Lr_scheduler: None, LambdaLR, MultiplicativeLR, StepLR, CosineAnnealingWarmUpRestarts
    - Optimizer: None, RMSprop, Adam, AdamW, SGD
    - Learning rate: $[2e-5, 2e-4]$
    - Batch size: {$16, 32, 64$}
    - Epochs: $[2, 5]$
    - Train: last layer vs + last transformer block vs fine-tuning

### Result

- EDA Results
  - An augmentation probability of $p = 0.1$ (10%) caused instability.
  - A lower probability, $p = 0.01$ (1%), contributed to more stable performance improvements.
- AutoML Results
  - Both SGD Optimizer and Partial Training consistently degraded performance, so they were excluded.
- Final Results
  - As shown in the figure below, the performance improved significantly:
    - **F1 Score**: $50.31$ (baseline: $48.41$)
    - **Exact Match**: $35.12$ (baseline: $31.94$)