---
layout: post
title:  "I2AM: Interpreting Image-to-Image Latent Diffusion Models via Bi-Attribution Maps"
summary: "The Thirteenth International Conference on Learning Representations (ICLR), 2025"
author: Junseo Park
contributors: "<span style='font-weight: bold; color: #ff5733;'>Junseo Park</span> and Hyeryung Jang"
date: '2024-10-2 14:35:23 +0530'
category: conference
thumbnail: /assets/img/posts/i2am_1.png
keywords: diffusion model, xai, image inpainting, super-resolution
permalink: /blog/i2am/
usemathjax: true
paper_link: "https://openreview.net/forum?id=bBNUiErs26&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2025%2FConference%2FAuthors%23your-submissions)"
image: "/assets/img/posts/i2am_1.png"
---

<div align="center" style="
  font-family: 'Times New Roman', Times, serif;
  font-size: 20px;
  font-weight: bold;
  color: #4a4a4a;
  padding: 20px;
  margin: 20px auto;
  border: 2px solid #e0e0e0;
  border-radius: 10px;
  background: linear-gradient(120deg, #f0f8ff, #ffffff);
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);">
  üöÄ "Interpreting Image-to-Image LDMs via Bi-Attribution Maps" üåü
</div>

  
<p></p>


### Context

- Large-scale diffusion models have achieved significant advancements in image generation, particularly through cross-attention mechanisms.
- While cross-attention has been extensively studied in text-to-image tasks *(e.g., DAAM)*, its interpretability in image-to-image (I2I) diffusion models remains underexplored. To address this, we introduce Image-to-Image Attribution Maps ($\text{I}^{2}\text{AM}$).

### Problem

- The interpretability of I2I diffusion models is insufficiently explored.
- Applying text-to-image attribution/attention methods to I2I models often results in uninterpretable maps due to the contextual continuity of reference images *(vs. DAAM)*.

### Proposed Method

- We propose a bidirectional attribution analysis guided by two key research questions:
  - **Q1**: ‚ÄúWhich regions of the generated image are most influenced by the reference image?‚Äù ‚Äî exploring how the model utilizes the reference image during generation.
  - **Q2**: ‚ÄúWhich parts of the reference image contribute most to the generated image?‚Äù ‚Äî assessing how well the model captures and transfers critical details from the input.

<p align="center">
<img src='/assets/img/posts/i2am_2.png'>
</p>

- $\text{I}^{2}\text{AM}$ aggregates cross-attention scores across time steps, attention heads, and layers to provide insights into feature transfer between images:
  - ULAM (Unified-Level Attribution Map): Integrates time, head, and layer scores to reveal overall trends.
  - TLAM (Time-Level Attribution Map): Tracks image generation processes over time.
  - HLAM (Head-Level Attribution Map): Analyzes attention score distributions across heads, highlighting diverse information capture.
  - LLAM (Layer-Level Attribution Map): Explains how input features are processed and transformed at each layer, guiding model performance improvement.
  - SRAM (Specific-Reference Attribution Map): Highlights regions in the reference image contributing to specific patches in the generated image.


### Result

- The top attribution map shows how different areas of the generated image are influenced by the reference image **(Q1)**. Conversely, the bottom map demonstrates how different regions of the reference image contribute to the generation process **(Q2)**, and the right map illustrates which reference information was extracted during the generation of that cell.

<p align="center">
<img src='/assets/img/posts/i2am_1.png'>
</p>

- $\text{I}^{2}\text{AM}$ demonstrates effectiveness across object detection, inpainting, and super-resolution tasks, successfully identifying key regions critical to output generation even in complex scenes.

<p align="center">
<img src='/assets/img/posts/i2am_3.png'>
</p>

<p align="center">
<img src='/assets/img/posts/i2am_4.png'>
</p>

- We also introduce the **Inpainting Mask Attention Consistency Score (IMACS)**, a novel metric that evaluates alignment between attribution maps and inpainting masks, showing strong correlation with existing performance metrics.


<p align="center">
<img src='/assets/img/posts/i2am_5.png'>
</p>

- Extensive experiments validate that $\text{I}^{2}\text{AM}$ supports model debugging and refinement, enhancing both the performance and interpretability of I2I models.



<p align="center">
<img src='/assets/img/posts/i2am_6.png'>
</p>